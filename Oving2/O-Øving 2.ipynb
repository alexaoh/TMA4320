{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Øving 2, TMA4320\n",
    "\n",
    "* **Veiledning:** Torsdag 16. januar, 0815-1000 i H3\n",
    "* **Innleveringsfrist:** Torsdag 23. januar, kl 1400\n",
    "* **Innleveringsmetode** Følgende to krav er nødvendig for godkjenning\n",
    "    1. Opplasting av Jupyter Notebook (individuelt) i Blackboard\n",
    "    2. Svare på Blackboardskjema for de tre kontrollspørsmålene i øvingen\n",
    "\n",
    "\n",
    "Vi skal i denne oppgaven bruke fikspunktiterasjon og spesifikt Newtons metode, først for skalar ligning og deretter for et system av ligninger.\n",
    "\n",
    "**Oppgave 1** Fikspunktiterasjon for skalare ligninger.\n",
    "Vi ser nå på polynomet\n",
    "\n",
    "$$\n",
    "     p(x) = x^5-3x+1\n",
    "$$\n",
    "som har 3 reelle nullpunkter, $x_1\\approx -1.3887919844072541828$, $x_2\\approx 0.33473414194335268708$\n",
    "og $x_3\\approx 1.2146480426984618040$. Vi bruker to omforminger av ligningen til fikspunktform\n",
    "1. $x=g(x)$ med $g(x)=\\frac13(x^5+1)$ (\"tilfeldig\")\n",
    "2. $x=g(x)$ med $g(x)=x-\\frac{x^5-3x+1}{5x^4-3}$ (Newton)\n",
    "\n",
    "I fikspunktiterasjonen vil vi ha en startverdi $x^{(0)}$ og en toleranse $\\mathrm{tol}$. Stoppkriteriet vi bruker er\n",
    "\n",
    "$$\n",
    "    |x^{(k)}-x^{(k-1)}| < \\mathrm{tol}\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Lag funksjoner *fiksit* og *newton* for hvert av tilfellene tar som input startverdien $x^{(0)}$ og toleransen tol, og returnerer approksimasjon til roten samt antall iterasjoner benyttet. Funksjonene skrives spesifikt for dette eksemplet. Sørg for at funksjonen avslutter dersom metoden ikke har konvergert etter *maxiter* iterasjoner.\n",
    "Test ut med $x^{(0)}=0$ og $\\mathrm{tol}=10^{-10}$.\n",
    "</div>\n",
    "\n",
    "**Kontrollspørsmål 1** Hvor mange iterasjoner bruker de to funksjonene med denne startverdien/toleransen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function fiksit() runs 100 iterations and gets the following approximate root: 0.3347341419433527\n",
      "The function newton() runs 4 iterations and gets the following approximate root: 0.33473414194335266\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Makes the code more readable when the functions are isolated, even though the task is to make fiksit \n",
    "#and newton specific to the two functions I have called g1 and g2. \n",
    "def g1(x): \n",
    "    return (1/3)*(x**5+1)\n",
    "\n",
    "def g2(x):\n",
    "    return x-((x**5-3*x+1)/(5*x**4-3))\n",
    "\n",
    "def fiksit(x0,tol, g):\n",
    "    assert tol>0\n",
    "    maxiter = 100 #for example\n",
    "    iter = 0\n",
    "    errest = 2*tol\n",
    "    while errest > tol:\n",
    "        iter += 1\n",
    "        x = g(x0)\n",
    "        errest = abs(x-x0)\n",
    "        x0 = x\n",
    "        if iter >= maxiter:\n",
    "            break;\n",
    "    return x,iter\n",
    "\n",
    "def newton(x0,tol,g):\n",
    "    assert tol>0\n",
    "    maxiter = 100 #for example\n",
    "    iter = 0\n",
    "    errest = 2*tol\n",
    "    while errest > tol:\n",
    "        iter += 1\n",
    "        x = g(x0)\n",
    "        errest = abs(x-x0)\n",
    "        x0 = x\n",
    "        if iter > maxiter:\n",
    "            break;\n",
    "    return x,iter\n",
    "\n",
    "#My two functions are exactly identical when the task is solved this way. \n",
    "#Wrong? I could do it differently by copying the code from g1 and g2 into  \"their respective\" functions.\n",
    "\n",
    "#Gjør kall til funksjoner som utfører eksperimenter her\n",
    "x0 = 0\n",
    "tol = 10**(-10)\n",
    "fiksit_x, fiksit_iter = fiksit(x0, tol, g1)\n",
    "newton_x, newton_iter = newton(x0, tol, g2)\n",
    "print(\"The function fiksit() runs\", fiksit_iter, \"iterations and gets the following approximate root:\", fiksit_x)\n",
    "print(\"The function newton() runs\", newton_iter, \"iterations and gets the following approximate root:\", newton_x)\n",
    "#Kontrollspm 1: Se nedenfor for svar på hvor mange iterasjoner hver av funksjonene bruker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oppgave 2.** Vi skal nå se på bruk av automatisk derivasjon for Newtons metode. Automatisk derivasjon er en teknikk som ble oppfunnet for ikke så veldig mange år siden. Det er en algoritme som kan ta en vilkårlig funksjon skrevet f.eks. i Python, og genererer fra denne automatisk den deriverte av funksjonen. Det er ikke det samme som symbolsk derivasjon for resultatet foreligger ikke som et symbolsk uttrykk. I dette kurset skal vi kun bruke ferdig implementert automatisk derivasjon. Det fins en pakke for dette formålet i Python som heter *autograd*. Vi kan ikke benytte 'vanlig numpy' når vi gjør autoderivasjon, men bruker en variant autograd.numpy, det er derfor denne vi nå må importere. I vinduet nedenfor demonstrerer vi bruken og har valgt å hente inn funksjonen *value_and_grad* fra *autograd*. Den kan brukes til å finne et tuple bestående av funksjonsverdi og derivert for gitte verdier av argumentet til funksjonen. Mye blir nok klart hvis du leser følgende eksempel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49999999999999994\n",
      "0.8660254037844387\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np   \n",
    "from autograd import value_and_grad \n",
    "\n",
    "def g(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "Dg = value_and_grad(g)\n",
    "G=Dg(np.pi/6.)\n",
    "print(G[0])\n",
    "print(G[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Med $g(x)=\\sin x$ får vi laget funksjonen Dg som returnerer tuplet $(g(x),g'(x))=(\\sin x, \\cos x)$.\n",
    "Som du vet er $\\sin\\frac{\\pi}{6}=\\frac12$ mens $\\cos\\frac{\\pi}{6}=\\frac12\\sqrt{3}\\approx 0.866$.\n",
    "\n",
    "Når man bruker *autograd* er datatypen til input viktig. Du bør for eksempel alltid definere tall som skal være flyttall med desimalpunktum, f.eks. skriv ikke *x0=1*, men *x0=1.0*. I motsatt fall kan du få kryptiske feilmeldinger.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**(a)** Lag en funksjon *gNewton* som tar tre inputargumenter: funksjonen $f(x)$ som en skal finne nullpunkter til, startverdien $x^{(0)}$, og toleransen tol. Returner approksimert rot og antall iterasjoner. </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**(b)** Test *gNewton* på funksjonen\n",
    "\n",
    "$$\n",
    " f(x) = \\frac13\\sqrt{1+\\sin(\\tanh x)}+\\cos x\n",
    "$$\n",
    "med startverdi $x0=\\{-2.0,2.0,4.0,8.0\\}$, $\\mathrm{tol}=10^{-10}$ og skriv ut rotapproksimasjon og antall iterasjoner i hvert tilfelle. \n",
    "\n",
    "</div>\n",
    "\n",
    "**Kontrollspørsmål 2** Hvor mange distinkte (forskjellige) røtter fikk du konvergens mot totalt med disse 4 startverdiene?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 = -2.0: Approximate root -1.7180861141125476. Iterations: 4.\n",
      "\n",
      "x0 = 2.0: Approximate root 2.0376244457683277. Iterations: 4.\n",
      "\n",
      "x0 = 4.0: Approximate root 4.24303676441417. Iterations: 5.\n",
      "\n",
      "x0 = 8.0: Approximate root 8.323364548870096. Iterations: 5.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fyll inn koden for b-spørsmålet her\n",
    "import autograd.numpy as np\n",
    "from autograd import value_and_grad\n",
    "\n",
    "def f(x):\n",
    "    # Fyll inn definisjon av den konkrete funksjonen f her\n",
    "    fval = (1/3)*np.sqrt(1+np.sin(np.tanh(x)))+np.cos(x)\n",
    "    return fval\n",
    "\n",
    "\n",
    "def gNewton(f, x0, tol):\n",
    "    # Fyll in kode for gNewton her\n",
    "    assert tol>0\n",
    "    maxiter = 100 #for example\n",
    "    iter = 0\n",
    "    errest = 2*tol\n",
    "    while errest > tol:\n",
    "        iter += 1\n",
    "        f_der = value_and_grad(f)\n",
    "        x = x0 - f(x0)/f_der(x0)[1]\n",
    "        errest = abs(x-x0)\n",
    "        x0 = x\n",
    "        if iter > maxiter:\n",
    "            break\n",
    "    return x,iter\n",
    "    \n",
    "\n",
    "# Utfør påkrevde tester under her\n",
    "x01 = -2.0\n",
    "x02 = 2.0\n",
    "x03 = 4.0\n",
    "x04 = 8.0\n",
    "tol = 10**(-10)\n",
    "gNewton_root1, gNewton_iter1 = gNewton(f, x01, tol)\n",
    "gNewton_root2, gNewton_iter2 = gNewton(f, x02, tol)\n",
    "gNewton_root3, gNewton_iter3 = gNewton(f, x03, tol)\n",
    "gNewton_root4, gNewton_iter4 = gNewton(f, x04, tol)\n",
    "print(\"x0 = \"+str(x01)+\": Approximate root \" + str(gNewton_root1) + \". Iterations: \" + str(gNewton_iter1) + \".\\n\")   \n",
    "print(\"x0 = \"+str(x02)+\": Approximate root \" + str(gNewton_root2) + \". Iterations: \" + str(gNewton_iter2) + \".\\n\")  \n",
    "print(\"x0 = \"+str(x03)+\": Approximate root \" + str(gNewton_root3) + \". Iterations: \" + str(gNewton_iter3) + \".\\n\")  \n",
    "print(\"x0 = \"+str(x04)+\": Approximate root \" + str(gNewton_root4) + \". Iterations: \" + str(gNewton_iter4) + \".\\n\")  \n",
    "#Kontrollspm 2: fikk konvvergens mot totalt 4 distinkte røtter med disse startverdiene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oppgave 3** Newtons metode for systemer av ligninger\n",
    "\n",
    "Nå skal vi gå litt videre i forhold til forelesningsmaterialet, og se på systemer av ligninger.\n",
    "Newton's metode kan fremdeles defineres, men må generaliseres. Et $n$-dimensjonalt system av ligninger kan skrives\n",
    "$$\n",
    "\\begin{array}{lcr}\n",
    "f_0(x_0,\\ldots,x_{n-1})&=&0\\\\\n",
    "f_1(x_0,\\ldots,x_{n-1})&=&0\\\\\n",
    "&\\vdots&\\\\\n",
    "f_{n-1}(x_0,\\ldots,x_{n-1})&=&0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "dvs $n$ ligninger $f_0,\\ldots,f_{n-1}$ for $n$ ukjente $x_0,\\ldots,x_{n-1}$. \n",
    "I kortspråk skriver vi dette kun som $\\mathbf{f}(\\mathbf{x})=0$.\n",
    "\n",
    "En sentral størrelse her er den såkalte\n",
    "Jacobi-matrisen. Hvis vi partiellderiverer $n$ funksjoner, hver med hensyn på $n$ variable, så blir det totalt\n",
    "$n^2$ funksjoner, dette er elementene i Jacobimatrisen\n",
    "\n",
    "$$\n",
    "D\\mathbf{f} = \\left[\n",
    "\\begin{array}{cccc}\n",
    "\\frac{\\partial f_0}{\\partial x_0} & \\frac{\\partial f_0}{\\partial x_1} & \\cdots & \\frac{\\partial f_0}{\\partial x_{n-1}} \\\\\n",
    "\\frac{\\partial f_1}{\\partial x_0} & \\frac{\\partial f_1}{\\partial x_1} &\\cdots & \\frac{\\partial f_1}{\\partial x_{n-1}}\\\\\n",
    "\\vdots                            & \\vdots                            &        &    \\vdots   \\\\\n",
    "\\frac{\\partial f_{n-1}}{\\partial x_0} & \\frac{\\partial f_{n-1}}{\\partial x_1} &\\cdots & \\frac{\\partial f_{n-1}}{\\partial x_{n-1}}\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Generaliseringen av Newton's metode er nå rett og slett: Gitt $\\mathbf{x}^{(0)}\\in\\mathbb{R}^n$ la\n",
    "\n",
    "$$\n",
    "    \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\left[D\\mathbf{f}(\\mathbf{x}^{(k)})\\right]^{-1}\\cdot \\mathbf{f}(\\mathbf{x}^{(k)})\n",
    "$$\n",
    "\n",
    "For å implementere denne kan vi definere $\\mathbf{f}$ som en funksjon som tar et numpy-array som input og returnerer et numpy-array. Så fins det en funksjon i pakken *autograd* som heter *jacobian* og som beregner Jacobimatrisen. Vi illustrerer hvordan vi bruker *jacobian* på funksjonen \n",
    "\n",
    "$$\n",
    "\\mathbf{f}(x_0,x_1) = \\left[\\begin{array}{c}x_0^2+x_1^2\\\\ x_0 x_1\\end{array}\\right]\\quad\\Rightarrow\\quad \n",
    "Df=\\left[\n",
    "\\begin{array}{cc}\n",
    "2x_0 & 2 x_1 \\\\ x_1 & x_0\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 4.]\n",
      " [2. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "\n",
    "def f(x):\n",
    "    return np.array([x[0]**2+x[1]**2,x[0]*x[1]])\n",
    "\n",
    "Df = jacobian(f)\n",
    "x=np.array([1.0,2.0])\n",
    "print(Df(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merk at kallet til *jacobian* gjøres bare én gang når *f* er definert. Etterpå kan *Df* brukes flere ganger med ulike inputargumenter.\n",
    "\n",
    "Nå litt om hvordan man implementerer Newton's metode. Inversmatrisen som figurere i iterasjonen gjør vi her enkelt ved å kalle en ferdigrutine i *numpy.linalg*, se nedenfor. Hvis vi definerer $\\delta=D\\mathbf{f}(\\mathbf{x}^{(k)})^{-1}\\cdot\\mathbf{f}(\\mathbf{x}^{(k)})$ kan vi skrive en Newtoniterasjon som\n",
    "1. Løs $D\\mathbf{f}(\\mathbf{x}^{(k)})\\delta = -\\mathbf{f}(\\mathbf{x}^{(k)})$ m.h.p. $\\delta$\n",
    "2. Sett $\\mathbf{x}^{(k+1)}=\\mathbf{x}^{(k)}+\\delta$\n",
    "\n",
    "I *numpy* fins en delpakke som heter *numpy.linalg* og i denne fins en funksjon som heter *solve*. Derfor kan vi løse ligningssystemer som i 1. ved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "\n",
    "A=np.array([[2,1],[3,1]])\n",
    "b=np.array([1,1])\n",
    "y=la.solve(A,b)\n",
    "print(y,la.norm(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi har også tatt med *la.norm* som beregner lengden til en vektor. Det trenger vi å gjøre når vi skal lage stoppkriterium i Newtoniterasjonen, det er naturlig å kreve at $\\mathrm{la.norm(delta)}<\\mathrm{tol}$.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**(a)** Skriv en funksjon sNewton som tar tre inputargumenter: funksjonen $\\mathbf{f}(\\mathbf{x})$ som en skal finne nullpunkter til, startverdien $\\mathbf{x}^{(0)}$, og toleransen tol. Returner approksimert rot og antall iterasjoner.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**(b)** Test ut sNewton på å finne nullpunkter til systemet\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "x_0^2 + x_1^2 &=& 1 \\\\\n",
    "x_0^3 - x_1 &=& 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "idet du velger toleransen slik at du kan gi et pålitelig svar på Kontrollspørsmål 3. Den første ligningen sier at eventuelle nullpunkter ligger på sirkelen sentrert i $(0,0)$ med radius 1. Den kubiske kurven $x_1=x_0^3$ fra den andre ligningen skjærer denne sirkelen nøyaktig to steder, så det fins akkurat 2 nullpunkter.\n",
    "</div>\n",
    "\n",
    "**Kontrollspørsmål 3.** I spørsmål **(3b)** angi det åttende desimalet etter komma for hhv $x_0$- og $x_1$-komponenten av roten (merk at det spiller ingen rolle hvilken av de to røttene du velger). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The approximate root is: [0.82603136 0.56362416] and the number of iterations are:  5\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "import numpy.linalg as la\n",
    "\n",
    "def f(x):\n",
    "    #Definer funksjonen f her, input er et numpy-array, output er også et numpy-array\n",
    "    fval = np.array([x[0]**2+x[1]**2-1, x[0]**3-x[1]])\n",
    "    return fval\n",
    "\n",
    "def sNewton(f,x0,tol):\n",
    "    #Skriv kode for Newtoniterasjoner for systemer med automatisk derivasjon her\n",
    "    assert tol>0\n",
    "    maxiter = 100 #for example\n",
    "    iter = 0\n",
    "    errest = 2*tol #Defined so that the while loop starts. \n",
    "    while errest > tol:\n",
    "        iter += 1\n",
    "        Df = jacobian(f)\n",
    "        A = Df(x0)\n",
    "        b = -f(x0)\n",
    "        delta = la.solve(A,b)\n",
    "        x = x0 + delta\n",
    "        errest = la.norm(delta)\n",
    "        x0 = x\n",
    "        if iter > maxiter:\n",
    "            break\n",
    "    return x, iter\n",
    "        \n",
    "\n",
    "#Gjør kall til sNewton og utfør påkrevde eksperimenter under her \n",
    "x0 = np.array([1.0,1.0])\n",
    "tol = 10**(-10)\n",
    "approx_root, iterations = sNewton(f,x0, tol)\n",
    "print(\"The approximate root is:\", approx_root, \"and the number of iterations are: \", iterations)\n",
    "#Kontrollspm 3: Det åttende desimalet i hhv x0- og x1-komponentene er 6, \n",
    "#Ifølge rettingen på kontrollSPM skal det åttende desimalet i x0 være 5. Hva er galt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}